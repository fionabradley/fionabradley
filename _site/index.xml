<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Fiona Bradley</title>
<link>https://www.fionabradley.com/index.html</link>
<atom:link href="https://www.fionabradley.com/index.xml" rel="self" type="application/rss+xml"/>
<description>Fiona Bradley</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Wed, 31 Jan 2024 13:00:00 GMT</lastBuildDate>
<item>
  <title>From novelty to the new normal: the current and future state of generative AI</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/from-novelty-to-the-new-normal/index.html</link>
  <description><![CDATA[ 




<p><em>Keynote presentation at the <a href="https://www.apla.asn.au/">Association of Australasian Parliamentary Libraries</a> Conference, held at the Parliament of NSW, 1 February 2024.</em></p>
<p>Hello – delighted to be invited to talk to you today. It’s been about 18 months since the latest rollercoaster of interest and hype in AI began. For some, rollercoasters are exciting and novel experiences, for others they are terrifying. For anyone trying to follow what’s happening in AI and what it means for how we access and use information and what it means to libraries, it’s much the same experience. It’s been challenging to separate what’s truly exciting from what’s terrifying. It can seem at times that the ups and downs of a handful of influential companies is having an outsized impact on everything from the state of the English language to jobs and stability of society. How much of this is real right now and how much is exaggerated? What does it mean for libraries and access to quality information?</p>
<p>I’ll give you my response right up front: While the current era of generative AI is still very new, <a href="https://www.tandfonline.com/doi/full/10.1080/24750158.2022.2101911">librarians have been encountering and working with some areas of AI</a> for a long time. This should give us confidence in the roles we can play in supporting ethical, trustworthy development and use of AI. With that said, researchers, librarians, policymakers are all still trying to figure out what impact AI has already had and will have on the ways we access, understand, and use information. Precisely because of our longstanding roles in providing access to quality information, developing information skills, information synthesis – we are also acutely aware of some of the threats to the information environment that can be amplified by new tools and technologies.</p>
<p>So if that’s what I believe our response should be, let’s scroll back a little bit to go over what generative AI is and what the big issues are. When we’re talking about AI in the context of libraries, we’re mostly talking about generative AI. There are hundreds of different methods and branches of AI, and generative AI is just one of them. It involves generation of text, audio, video using models that are trained on large amounts of inputs. One of the first, and most significant examples of this was a research project called <a href="https://image-net.org/">ImageNet</a>, led by Fei-Fei Li. Projects like ImageNet, and later on the wider availability of tools like TensorFlow and other datasets laid the groundwork for an explosion of interest in research on image classification and text analysis. These have applications in research areas like developing autonomous lawn mowers, to identifying plant species. Over the last decade or so, many libraries, especially national libraries, have been doing work on the concept of collections as data. <a href="https://collectionsasdata.github.io/statement/">Collections as data</a> is about making available catalogue records and sometimes also digitised collections, often as linked data, and seeing what uses other researchers can make of them. Another popular application of machine learning in library collections is the <a href="https://huggingface.co/datasets/TheBritishLibrary/blbooks">use of OCR to recognise handwriting in old manuscripts</a>. And the National Library Board in Singapore has just launched a <a href="https://www.nlb.gov.sg/main/about-us/press-room-and-publications/media-releases/2023/2024-Year-in-Preview">new pilot</a> and immersive experience called StoryGen that,</p>
<blockquote class="blockquote">
<p>“uses Generative AI to transform stories in text form into a visual and multimedia experience. Users can exercise their creativity to present well-loved stories in their own ways. For example, they can see a scene of the boy and the garfish in <em>Sejarah Melayu</em> come alive through images generated through AI, or take it further and present it in another genre like sci-fi.”</p>
</blockquote>
<p>A further area of AI that is used a lot of libraries is NLP – Natural Language Processing – which can include looking at unstructured documents for popular terms, trends, sentiment analysis etc. In my own research, I frequently work with <a href="https://spacy.io/">Named Entity Recognition</a> to extract meaning from texts and use them for further analyses to identify relationships between different people and organisations. These sorts of methods are also used in some of the literature searching tools that libraries subscribe to and provide to our clients. This is the first point I’d like you to keep in mind – AI has been used in some libraries and systems we use for some time. What’s important is to understand how AI is implemented in these services, and how our personal and browsing data is used.</p>
<p>Something all these projects have in common is that they all require larger and larger sets of inputs, or training data, to make models more accurate. <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">Examples of some specialised datasets can be found on Wikipedia.</a> These datasets include research publications, government data, social media, music and movie ratings, and other sources for learning. Overcoming barriers to accessing large corpora of data, text and other inputs for non-profit research is one of the reasons why librarians and researchers in some countries, especially in Europe, have been advocating for amendments to copyright law to create more flexible exceptions and limitations, including for <a href="https://eare.eu/tdm/">text and data mining</a>. Text and data mining, or TDM, is effectively a pre-requisite for getting legal access to a large corpus for analysis. And this is the second point I’d like you to keep in mind – what impact do debates about licensing and copyright mean for AI and what we, and AI companies alike can do in the future?</p>
<p>Generative AI launched into the public consciousness after the launch of OpenAI’s ChatGPT, Google’s Bard, Meta’s LLaMA and many other large language models. These launches were followed by a torrent of other product announcements and add-ons. Lately it can seem like AI is everywhere. I bought a new rice cooker last year that says it’s got AI. I’m pretty sure it’s just a new term for fuzzy logic. Generative AI has moved very quickly from a standalone service to being integrated with everyday word processing, spreadsheet and office software. It feels like there is a real drive to normalise and integrate AI into everything, very quickly. Yet, there is a sense partially because of a “<a href="https://doi.org/10.2307/j.ctvt1sgc0.4">black-boxing</a>” effect, and partially because skills and literacies take time to develop, that we don’t really understand how these technologies work, why and when they make mistakes, and what the consequences are.</p>
<p>Some recent claims that turned out to be hype at best, misleading at worst:</p>
<ul>
<li><p>The <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">call for a six month pause</a> on development of large language models warning about threats to society, backed by a group of tech founders. Some of whom stood to profit from such a pause. The letter was heavily criticised by <a href="https://www.dair-institute.org/blog/letter-statement-March2023/">AI ethicists</a>.</p></li>
<li><p>Scary threats about the potential near future of sentient AGI and frontier AI, much of this covered at the <a href="https://www.gov.uk/government/topical-events/ai-safety-summit-2023">AI Safety Summit</a> in the UK and the <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Declaration</a> (which the <a href="https://parlinfo.aph.gov.au/parlInfo/search/display/display.w3p;query=Id%3A%22media%2Fpressrel%2F9450844%22">Australian government has signed</a>)</p></li>
<li><p>Claims that generative AI would be able to summarise all of human knowledge and take out menial work of data cleansing analysis, reading etc analysing and interpreting research eg for policy. There are plenty of tools available now that claim to be able to do just this. Quality varies. Widely.</p></li>
</ul>
<p>But setting aside the hype, it’s important to remember the reality – as many researchers have pointed out, including the <a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">DAIR</a>, Distributed AI Research Institute, there are <a href="https://www.scientificamerican.com/article/we-need-to-focus-on-ais-real-harms-not-imaginary-existential-risks/">real world harms that exist now</a>. Some of these include harms to people who work for low pay to manually classify images and data. Other harms include creators whose work is used without attribution or compensation. Fei-Fei Li, whose pioneering image classification work on ImageNet I mentioned earlier, <a href="https://www.theguardian.com/technology/2023/nov/05/ai-pioneer-fei-fei-li-im-more-concerned-about-the-risks-that-are-here-and-now">reminds us that</a>,</p>
<blockquote class="blockquote">
<p>“First, to be clear, AI is”promising” nothing. It is people who are promising – or not promising. AI is a piece of software. It is made by people, deployed by people and governed by people.”</p>
</blockquote>
<p>Generative AI as it currently exists is very good at some things, it’s also not very good at some things. Knowing the difference is key. AI is being used extensively in research for specific tasks like better <a href="https://www.technologyreview.com/2023/07/11/1076067/weather-forecasting-is-having-an-ai-moment/">weather predictions</a>, <a href="https://www.nature.com/articles/s42256-020-00287-7">meta analyses</a> of past research studies, and <a href="https://journals.sagepub.com/doi/full/10.1177/02683962211048201">literature reviews</a>. Mostly, <a href="https://shows.acast.com/theeconomistbabbage/episodes/babbage-how-ai-promises-to-revolutionise-science">AI is speeding up work</a> that used to be done manually by junior researchers. In my own research, it speeds up the process of ‘reading’ many texts and looking for the names of organisations and people. But no matter how generative AI is applied, expert knowledge of a field and its tools are still essential. Generative AI is nowhere near replacing human insight yet. Most of you have probably tried out general purpose writing tools like Bard or ChatGPT by now. Maybe you’ve tried generating images in Google Sheets or Midjourney, or attended an event with live captioning. Think about your own experience in using those tools. They are ok for an outline or some questions that you may have previously entered into a search engine. Their answers were probably ok, but not great. Instant image translations might get the gist, but they can also be wildly inaccurate. At the same time, some more specialised areas of AI research and industry have been making incredible progress in improving accessibility for people with visual impairments and other sensory loss. For many reasons therefore, has been some recent analysis suggesting that <a href="https://www.economist.com/finance-and-economics/2024/01/07/what-happened-to-the-artificial-intelligence-investment-boom">business interest in generative AI is waning</a> a little bit, for now,</p>
<blockquote class="blockquote">
<p>“Big tech firms love the technology, but are going to struggle to find customers for the products and services that they have spent tens of billions of dollars developing. It would not be the first time in recent history that technologists have overestimated demand for new innovations. Think of the metaverse.</p>
<p>The second interpretation is less gloomy, and more plausible. Adoption of new general-purpose tech tends to take time.”</p>
</blockquote>
<p>For regulators also, there’s a lot at stake. I started <a href="https://library.ifla.org/id/eprint/2588/">researching the regulatory environment around AI in 2019</a> and the changes since that time have been rapid and complex. Getting the balance right between allowing industries and novel applications to flourish, while ensuring there are guardrails around the potentially damaging and harmful applications of AI is a very difficult task. <a href="https://journals.sagepub.com/doi/10.1177/0306312713508669">Anticipatory governance</a> is one way of thinking about this challenge – regulate too early and you may stifle innovation. Too late, and regulators can only play catch-up. At a higher political level are debates about the influence and power a small number of technology companies can or may wield and what this says about the state of geopolitics today and in the future. So significant is this shift that some suggest a major step change is needed in how policymakers engage with such actors when designing regulation,</p>
<blockquote class="blockquote">
<p>“Like past technological waves, AI will pair extraordinary growth and opportunity with immense disruption and risk. But unlike previous waves, it will also initiate a seismic shift in the structure and balance of global power as it threatens the status of nation-states as the world’s primary geopolitical actors. Whether they admit it or not, AI’s creators are themselves geopolitical actors, and their sovereignty over AI further entrenches the emerging”technopolar” order—one in which technology companies wield the kind of power in their domains once reserved for nation-states.” (<a href="https://www.foreignaffairs.com/world/artificial-intelligence-power-paradox">Bremmer and Suleyman, 2023</a>)</p>
</blockquote>
<p>But we’re not yet there. To date, concepts like <a href="https://www.industry.gov.au/news/australian-governments-interim-response-safe-and-responsible-ai-consultation">safe and responsible</a>, ethical, trustworthy, transparent AI have emerged as policy responses. Australia has policy and regulation at the state and federal level and has consulted widely on some of these. I contributed to two of <a href="https://read.alia.org.au/artificial-intelligence">ALIA’s responses</a> last year together with colleagues from across the sector. An <a href="https://www.industry.gov.au/news/australian-governments-interim-response-safe-and-responsible-ai-consultation">interim response</a> to one of the consultations on responsible AI was published in mid-January 2024. Topics for libraries to pay attention include intentions to further amend privacy legislation, and to introduce regulations around misinformation and disinformation. <a href="https://www.theguardian.com/australia-news/2024/jan/23/chatgpt-in-australian-schools-what-you-need-to-know-law-changes">Expectations that classroom teachers will be able to provide guidance to students</a> on the use of generative AI are well-meaning, but there is still a critical need to provide skills to educators, teachers and librarians to do this with confidence. This is a skillset that is needed across the entire library sector.</p>
<p>A particular challenge when it comes to generative AI, as mentioned previously, is that a lot of attention is on the actions of a small number of companies – Alphabet/Google, Meta, and OpenAI, which has a significant investment by Microsoft. Yet it’s important to note that many of the questions about regulating AI are not new. They are the same ones, involving many of the same players, when we think about questions of <a href="https://www.infrastructure.gov.au/department/media/news/new-disinformation-laws">social media</a>, <a href="https://www.acma.gov.au/news-media-bargaining-code">news media bargaining</a>, <a href="https://www.theguardian.com/australia-news/2023/nov/22/australia-social-media-ai-laws-crackdown-esafety">eSafety</a> and so on. At the centre of so many debates AI are the same questions that come up time and time again that are fundamental to us as librarians, our users, and the people that rely on quality information to make decisions:</p>
<ul>
<li><p>How do we maximise the benefits of new technologies and means of disseminating information, while reducing the harms?</p></li>
<li><p>Who gets to profit from the work of others? This question is at the heart of many lawsuits launched by media companies, artists, writers, and rightsholders. OpenAI have acknowledged that <a href="https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai">ChatGPT would not be possible to create</a> if they had to licence (and pay for) all the content that was ingested, legally or otherwise.</p></li>
<li><p>How do we make sure that legitimate uses for research, education, news – will not be damaged by efforts to deal with large <a href="https://www.theverge.com/2023/8/7/23823046/openai-data-scrape-block-ai">scrapers</a>?</p></li>
</ul>
<p>One of the other issues that we’ve seen before that has come up again in questions about AI is about the use of algorithms. Again, many librarians have some familiarity with algorithms. Many of the databases and search engines we subscribe to for access to news reports or research publications and library catalogues have made use of recommender systems and ranking algorithms for many years. These systems work when well the underlying information is well described, has good metadata, and has been configured well. But when we add AI, there is a concern that volume of information being generated by these tools combined with a tendency of some, but not all, social media algorithms to <a href="https://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content">recommend more extreme content</a> has many worried that we could start to see a massive increase in the creation and dissemination of misinformation. This is setting off alarm bells especially in 2024 which is being called one of the most <a href="https://www.economist.com/the-world-ahead/2023/11/13/2024-will-be-stressful-for-those-who-care-about-liberal-democracy">significant years for electoral democracies</a>. So how is our knowledge of library databases and catalogues helpful here? It gives us a strong appreciation of the fact that attempts by regulators and activists to push companies to provide explainable AI or algorithmic transparency are <a href="https://www.fionabradley.com/posts/ai-future-information-literacy-information-ethics/">quite difficult to achieve in practice</a>.</p>
<p>So I said at the beginning that libraries have a lot to bring to the table when it comes to shaping the future of ethical and trustworthy AI. Some of the areas libraries are working on:</p>
<ul>
<li><p>Bringing together experience in media and information literacy, and how this adapts to AI literacy</p></li>
<li><p>Experience in copyright reform, licensing, and TDM to shape a legal framework for AI that respects creators while ensuring sufficient flexibility for research, education, and related uses</p></li>
<li><p>Experimenting with AI – in collections, building new experiences for library users, and designing new ways to interact with services. In the <a href="https://www.ifla.org/units/ai/">IFLA AI Special Interest Group</a>, we have been <a href="https://www.ifla.org/g/ai/generative-ai/">developing resources</a> to help librarians to learn more about AI. <a href="https://github.com/AI4LAM">AI4LAM</a> is another active community with many members in Australia. If you have the time, I encourage you to have a look and try out a few tools.</p></li>
</ul>
<p>In a recent government consultation on responses to <a href="https://consult.industry.gov.au/supporting-responsible-ai">responsible AI</a>, <a href="https://read.alia.org.au/joint-submission-library-and-information-service-organisations-safe-and-responsible-ai-inquiry">ALIA and other organisations</a> called out the need for AI literacy, a joined up approach across sectors, called for libraries to be involved, and to consider impacts on First Nations communities and explore opportunities for labelling, citation, and acknowledgement when works are AI generated.</p>
<p>Open Access Australasia’s <a href="https://www.aph.gov.au/DocumentStore.ashx?id=11f16213-1a8a-41da-b51a-791638f493f6&amp;subId=745274">response to a consultation on generative AI</a> in education noted impacts on copyright reform – we don’t have TDM exceptions but <a href="https://www.theguardian.com/technology/2023/apr/19/google-calls-for-relaxing-of-australias-copyright-laws-so-ai-can-mine-websites-for-information">big tech wants them</a>, licensing, potential unintended consequences for open access to research, research integrity, and the concentration of power and resources among a small number of companies.</p>
<p>The question of trusted information is significant. When it comes to integrity, we’re already seeing AI be used for different purposes. On one hand, AI is being used to help detect research fraud, such as image manipulation. On the other, it’s now trivial for anyone to edit their photos to erase elements they don’t like. As <a href="https://fediscience.org/@timnitGebru@dair-community.social">Timnit Gebru</a> said on Mastodon just last week, responding to a a quote within a press release from Google Pixel,</p>
<blockquote class="blockquote">
<p>“Google’s new phones now use AI to let you edit photos to a degree never seen before, exchanging sad faces for happy ones and overcast afternoons for perfect sunsets.” Ok so do we want photos to remind us of that day, or makeup how the day actually went?</p>
</blockquote>
<p>It’s not so far to jump from that scenario to the fears many have about the potential for deepfakes to be manufactured on a grand scale for more sinister purposes, such as to influence the outcomes of an election. This is where debates about how to use technology for good comes in. For example, there’s a lot of debate about how to apply <a href="https://www.brookings.edu/articles/detecting-ai-fingerprints-a-guide-to-watermarking-and-beyond/">watermarks</a> or authoritative metadata to images, documents, and other digital objects to track the provenance and when it is altered.</p>
<p>A lot of the other work I do is in advocating for open access to research publications, so they can be accessed by policymakers, researchers outside universities, and the general public. One of the debates we are having in that community is how do we also help support access to trusted information, and ethical reuse. Again, this comes back to a conversation about <a href="https://upstream.force11.org/large-language-publishing/">who should benefit</a> from the efforts of other researchers – what impacts might this have for licensing, use of open content, and so on. How do we use these discussions to also further issues around integrity and trust in research and quality information. It’s very early days and there are no clear answers yet. A point that is regularly made by decisionmakers at all levels is that relationships remain important in helping people to find and make sense of the right, trusted information – and this is where librarians remain essential to providing expertise to help people find and understand sources.</p>
<p>As I quoted before, Fei-Fei Li said something that is true of both AI and every technology that has ever been built: AI is, “made by people, deployed by people and governed by people.” This also means AI is neither all good or all bad and should give us confidence that there are ways we can use it for good, and ways we can work together across our libraries and across our sector to shape it. We’ve been using some aspects of AI in our systems and services for a while, but there is still much more to learn about how AI is applied and the intersections with copyright and licensing that will impact what we can provide, to whom, and how much it might cost. Regulation is developing quickly, and we have a lot to contribute to debates about how to create guardrails around AI to ensure it is designed and used ethically, and protects rather than reduces privacy. We don’t know quite yet where generative AI tools will be another 18 months on from now, but one thing for sure is that they will be everywhere which gives us time to experiment. What was novelty not so long ago is quickly becoming the new normal.</p>



 ]]></description>
  <category>talks</category>
  <guid>https://www.fionabradley.com/posts/from-novelty-to-the-new-normal/index.html</guid>
  <pubDate>Wed, 31 Jan 2024 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Reading around the thesis</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/reading-around-the-thesis/index.html</link>
  <description><![CDATA[ 




<p>I’m often asked, such as in a recent webinar on research for ALIA, “how is your thesis going, anyway?”. Depending on the day I’ll either respond, “still going”, or launch into a long-winded overview of my latest observations. Fitting a part-time PhD around a day job and other professional commitments is not something to take on lightly.</p>
<p>Con asks, <a href="https://flexnib.com/2023/06/16/books-to-read/">how do you find books to read?</a> My research spans some very fast moving topics and there is a lot to keep up with. At the same time, I have been warned to not read too much. It’s a balancing act especially when getting up to speed with a new area quickly.</p>
<p>Many reading sources come from traditional research advice: find a key scholar or work in a field, read them, and citation chain to identify other relevant materials. <a href="https://www.foreignaffairs.com/">Foreign Affairs</a>, <a href="https://www.economist.com/culture/2022/12/06/these-are-the-economists-best-books-of-2022">The Economist</a>, <a href="https://lareviewofbooks.org/">LA Review of Books</a> and <a href="https://www.aldaily.com/">Arts &amp; Letters Daily</a> are mainstays for book reviews. ISA’s annual <a href="https://www.isanet.org/Programs/Awards/HR-Best-Book">human rights book award</a> is also worth checking.</p>
<p>Social media, especially Twitter, has also helped me find many relevant books. After Musk’s acquisition of Twitter, I set up a profile on Mastodon and combed through every account I followed to subscribe to newsletters and feeds in case their profile disappeared. Yet most scholars I follow have not migrated away from Twitter. The resulting information overload in following and digesting so many sources to find relevant information is very much felt.</p>
<p>At the same time, while there is danger in reading too much, I am a great fan of ‘reading around’ to have a better sense of history, context, and culture especially when working on somewhat technical, sometimes very dry research topics. This also provides an opportunity to introduce a broader range of sources into the mix, or simply ideas that run parallel to a research question. Some recent books that I’ve enjoyed in this vein:</p>
<p>Jing Tsu, <em>Kingdom of characters: The language revolution that made China modern</em><br>
Megan Walsh, <em>The Subplot: What China is reading and why it matters</em><br>
Frank Dikötter, <em>China after Mao<br>
</em>Jenny Odell, <em>How to do Nothing</em></p>
<p>In the era of generative AI where many of the conversations about the use of text tools to effectively game learning is leading to a very narrow approach where students only want to be exposed to ideas that have the greatest utility (ie, ‘is this on the test?’), I find it essential to make time for reading broadly and for fun. Even if the result is that the thesis is “still going”. It will all get done, one day.</p>



 ]]></description>
  <category>writing</category>
  <guid>https://www.fionabradley.com/posts/reading-around-the-thesis/index.html</guid>
  <pubDate>Fri, 16 Jun 2023 14:00:00 GMT</pubDate>
</item>
<item>
  <title>AI and the future of information literacy and information ethics</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/AI-future-information-literacy-information-ethics/index.html</link>
  <description><![CDATA[ 




<section id="ai-and-the-future-of-information-literacy-and-information-ethics" class="level2">
<h2 class="anchored" data-anchor-id="ai-and-the-future-of-information-literacy-and-information-ethics">AI and the future of information literacy and information ethics</h2>
<p><em>A few thoughts in advance of a panel talk being hosted 21 February 2023 by ALIA: <a href="https://www.alia.org.au/EventDetail?EventKey=AI001">AI, libraries, and the changing face of information literacy</a>. See you there.</em></p>
<p>Chatbots: Pity the chatbot. The derided “computer says no” tool that seemed to be known more for blocking direct human interaction with shops, banks, airlines, and insurance companies has finally found affection, and an extraordinary amount of free publicity, via OpenAI’s ChatGPT.</p>
<p>Released in late 2022, the latest version of ChatGPT 3.5 is billed as a limited research beta. How long it remains freely available is anyone’s guess. The tool is said to have <a href="https://www.vox.com/future-perfect/23591534/chatgpt-artificial-intelligence-google-baidu-microsoft-openai">triggered a new AI race</a> with competitors including Google, Microsoft, and Baidu (more a coincidence of timing than by design, I would say), reflection on <a href="https://www.tandfonline.com/doi/abs/10.1080/24750158.2022.2101911?journalCode=ualj21">AI regulation</a>, and a fair amount of existential angst about what makes writing innately human, truth, and ethics.</p>
<p>We probably won’t be talking about ChatGPT a few months from now. But we will almost certainly be talking about how generative text and creative AI tools are embedded and largely invisible in existing everyday activities and applications. If we think we have a problem with ‘<a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674970847">black-boxing</a>’ and non-transparent algorithms now things are about to get a whole lot more complicated. Some of these innovations will save time and effort and be genuinely fun and interesting to use. Others will further entrench inequities in the labour required to remove abhorrent content from training data, bias, and a lack of respect for creators’ rights.</p>
</section>
<section id="ai-ethics-or-why-this-stuff-keeps-people-up-at-night" class="level2">
<h2 class="anchored" data-anchor-id="ai-ethics-or-why-this-stuff-keeps-people-up-at-night">AI ethics: or why this stuff keeps people up at night</h2>
<p>AI ethics captures a vast range of concerns. Even within the library and information profession, whose very foundation is built on the value of access to quality information and patron privacy, there is insufficient debate about what new technologies and methodologies like AI and ML mean for services and the people that library serve. Privacy of personal data is affected by harvesting for training data, bias is common, and algorithmic decision-making that goes wrong can have major consequences.</p>
<p>A growing ethical debate concerns an apparent lack of respect for creator rights when ingesting content and creating training models. ChatGPT has apparently <a href="https://www.reddit.com/r/AO3/comments/zl72bn/update_ao3s_response_to_sudowritesai_scraping/">ingested the contents of Archive of our Own</a>, a prominent fanfic archive, without the consent of its authors. Reddit and Wikipedia are also thought to be major sources that have helped create ChatGPT’s generic English writing style. It seems any open content on the web is fair game for these models, regardless of whether authors have morally consented to their work being used in this way. “Just because you can, doesn’t mean you should” (cf: Jurassic Park), of course. Even more controversial is the <a href="https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060">use of images for generative creative works and art</a>. Or the use of AI to replace artists <a href="https://www.pcmag.com/news/netflix-taps-ai-to-generate-anime-backgrounds-rather-than-hire-humans">working on the backgrounds of an anime series</a>. Expect to see more ethical and legal issues in this space and <a href="https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060">a lot more lawsuits</a> as generative creative AI tools begin to be integrated directly into consumer software as well.</p>
<p>As for whether a tool that does not respect creator rights can itself be an author, the answer already seems clear cut: <a href="https://www.science.org/doi/10.1126/science.adg7879">no</a>.</p>
<p>For libraries, ethical issues in AI and ML ought to trigger a resurgence in information literacy. I have said frequently over the years that I cannot understand why a media and information literacy course is not <em>the</em> most popular and oversubscribed course at every university (and school and public library). I mean <a href="https://www.notion.so/checkpleasecc/Check-Please-Starter-Course-ae34d043575e42828dc2964437ea4eed">media and information literacy skills</a> that deconstruct sources of knowledge, invested interests, and allow people to critically appraise the veracity of what they are reading, viewing, and hearing. I often reflect on conversations with colleagues around 2015 from Ukraine, Romania, Finland, Denmark that were ringing the alarm bell about a lack of appreciation for debate, democratic values, and truth. As we have seen in Europe, the US and elsewhere in the years since, these values have been sorely tested. <a href="https://www.theguardian.com/world/2020/jan/28/fact-from-fiction-finlands-new-lessons-in-combating-fake-news">Finland’s forethought to invest in ensuring young people have these skills</a> in the face of endemic misinformation has proved to be visionary.</p>
<p>A technique used by autocrats and others to drown out genuine information is called “<a href="https://press.princeton.edu/books/hardcover/9780691178868/censored">flooding</a>”. With the rise of more widely available generative AI text tools, I worry that the bots are about to get a lot more ammunition. Each technological development reduces the costs and time required to create an awful lot of havoc. It is trivial to create a bot army on any given issue and drown out legitimate voices online on social media platforms and in search engine results. Such tactics are not unique to autocracies. Steve Bannon’s “<a href="https://edition.cnn.com/2021/11/16/media/steve-bannon-reliable-sources/index.html">flood the zone</a>”, anyone? Search engines are already struggling to cope with filtering out low quality SEO generated content. Add in a barrage of sort-of-correct-sounding, intersparsed with incorrect AI generated text, fake references, deepfake images and video and it will be even harder to find reliable information, especially in a crisis.</p>
<p><a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">AI hallucinations</a> are a longstanding phenomenon, but are said to be accelerating with the development of large language models. What ChatGPT and other LLMs don’t ‘know’ based on their training data they may just make up. As is evident when we see fake citations and other made up information. Basic errors in fact are one thing. But what will the response be when these errors are about people and incorrect facts <a href="https://twitter.com/random_walker/status/1625879796638003200">could be seen as defamatory</a>. Years on from the Google News Spain case and right to be forgotten, how do we ask a language model to ‘forget’ what it doesn’t really know?</p>
<p>Garbage in, garbage out, as the saying goes. No wonder some say ChatGPT is just a <a href="https://futurism.com/artificial-intelligence-automated-mansplaining-machine">mansplainer</a>. These risks might sound farfetched. But consider OpenAI’s own comments a couple of years ago in the context of EU regulations on upload filters and potential to alter/censor content for European audiences:</p>
<blockquote class="blockquote">
<p>“…the designers of the futuristic OpenAI system, which can create limitless deepfakes for text including negative and positive customer reviews, spam and fake news that are sufficiently persuasive to be plausible as human creations, decided to raise the alarm. Indeed, perhaps not surprisingly, the public has already been alerted to the fact that the technology is too dangerous to release for fear of its potential abuse (<a href="https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction">The Guardian News, February 14, 2019</a>).” (<a href="https://doi.org/10.1080/13600869.2020.1733760">Romero Moreno, 2020, p.&nbsp;174</a>)</p>
</blockquote>
<p>Too dangerous to release. And yet, here we are.</p>
<p><a href="https://www.theguardian.com/books/2019/aug/18/behind-the-screen-sarah-t-roberts-review">Like content moderation before it</a>, ChatGPT has also revealed troubling realities about how tools are trained so that we don’t see the most hateful, violent, illegal content. <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/">This labour is cheap</a> to the companies that use it, but comes at a very high psychological cost to people that undertake the work.</p>
<p>To address questions of <a href="https://www.theguardian.com/technology/2023/feb/09/googles-bard-demo-what-went-wrong-chatgpt-chatbots-ai">bias</a>, among other intentions, there has been a groundswell of regulatory activity in China and the EU to require companies to open up their algorithms. <a href="https://carnegieendowment.org/2022/12/09/what-china-s-algorithm-registry-reveals-about-ai-governance-pub-88606">Sina Weibo’s hot search</a> is one algorithm that has been highly scrutinized. While many results are genuine and organic, the list can be gamed. Members of fanclubs, for instance, spend a great deal of effort trying to influence hot searches about their favourite entertainers while “antis” try to damage their idol’s perceived competitors. Successive crackdowns on toxic fan culture, <a href="https://www.reuters.com/world/china/chinas-cyberspace-regulator-says-internet-clean-up-campaign-sees-positive-2022-03-10/">campaigns to “clean up” the internet</a>, an overriding emphasis on security, as well as regulations about data protection all provide context for the <a href="https://www.dw.com/en/china-sets-new-rules-for-internet-algorithms/a-58999473">introduction of regulations to publish algorithms</a> in mid-2021. <a href="https://carnegieendowment.org/2022/12/09/what-china-s-algorithm-registry-reveals-about-ai-governance-pub-88606">Early reviews of the approach in China</a> suggests that while regulation has helped to push for some transparency, algorithms are so complex and what companies report at times so vague that it is unclear what the regulations can achieve.</p>
<p>This could be a preview of what happens in other regions that go down the same regulatory road. For different reasons, the <a href="https://algorithmic-transparency.ec.europa.eu/index_en">EU is rolling out an approach</a> as part of the Digital Services Act. The EU approach is motivated by concerns about the influence of largely US-based multinational platforms used by EU citizens, and taps into the EU’s longstanding orientation as a <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3501410">regulatory leader</a>.</p>
</section>
<section id="is-it-all-doom-and-gloom-no" class="level2">
<h2 class="anchored" data-anchor-id="is-it-all-doom-and-gloom-no">Is it all doom and gloom? No</h2>
<p>Yes there’s a lot to worry about, and a lot to get right to ensure that the future of generative, large language model AI is ethical and trustworthy. These are keywords in the plans and minds of many regulators, and people having a go in using <a href="chat.openai.com/">OpenAI’s ChatGPT</a>, <a href="https://www.bing.com/search?q=Bing+AI&amp;showconv=1&amp;FORM=hpcodx">Bing</a>, <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">Google “Bard”</a>, the forthcoming <a href="https://ai.baidu.com/">Baidu “Ernie Bot”</a>, and whatever tools come next.</p>
<p>The more likely scenario in the short-term is that we are going to see a lot more AI whether real or just hype. Be extra critical of apps ending in the domain name .ai. We will see new tools embedded into search products, bibliographic tools, word processors, spreadsheets and the like. Some of this is already happening - for instance subjects in some bibliographic tools are applied using machine learning, and many recommender tools use ML. If you are familiar with the Editor function in Microsoft Word or tools like Grammarly expect to see more like that. Much of this will be invisible. It will make it easier to correct your spelling and get help on how to use a particular function. I’d honestly be delighted if a company put all their attention into a fully functional and automated meeting scheduler that can account for internal and external people, hybrid and online formats, and timezones. Just an idea, Microsoft.</p>
<p>We are far from being put out of a job. Or losing our humanity. Not in 2023. I am convinced there’s more to do than ever for those who research these issues and for those working in libraries to ensure that we ensure the next wave of media and information literacy is fit for the algorithmic age, that research data is <a href="https://cacm.acm.org/magazines/2021/12/256932-datasheets-for-datasets/abstract">correctly described</a> so that it can be ethically reused, and that we collaborate closely with researchers and integrity staff to ensure matters of authorship, attribution, accuracy in scholarly publishing in the face of new technologies is well understood.</p>


</section>

 ]]></description>
  <category>writing</category>
  <guid>https://www.fionabradley.com/posts/AI-future-information-literacy-information-ethics/index.html</guid>
  <pubDate>Sat, 18 Feb 2023 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Celebrating Open Access Week 2022</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/open-access-week-2022/index.html</link>
  <description><![CDATA[ 




<p>Ginny Barbour, Clare Thorpe and I wrote for <a href="https://campusmorningmail.com.au">Campus Morning Mail</a> about climate, Open Access Week, UNESCO and more: <a href="https://campusmorningmail.com.au/news/open-access-for-climate-justice/"><code>Open Access for Climate Justice</code></a>: the opportunity to publish openly and affordably are essential contributors to solving global challenges.</p>



 ]]></description>
  <category>writing</category>
  <guid>https://www.fionabradley.com/posts/open-access-week-2022/index.html</guid>
  <pubDate>Wed, 19 Oct 2022 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Contribution of open access to the UN Sustainable Development Goals</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/contribution-of-open-access-to-the-un-sustainable-development-goals/index.html</link>
  <description><![CDATA[ 




<section id="contribution-of-open-access-to-the-un-sustainable-development-goals" class="level2">
<h2 class="anchored" data-anchor-id="contribution-of-open-access-to-the-un-sustainable-development-goals">Contribution of open access to the UN Sustainable Development Goals</h2>
<p><em>Talk presented to <a href="https://oaaustralasia.org/">Open Access Australasia</a>, 31 August 2021</em></p>
<p>In September 2015, after more than three years of negotiations the Member States of the United Nations adopted the <a href="https://sdgs.un.org/2030agenda">UN 2030 Agenda for Sustainable Development</a>. The Agenda is an inclusive, integrated framework of 17 Sustainable Development Goals (SDGs) spanning economic, environmental and social development. They lay out a plan for all countries to actively engage in making our world better for its people and the planet.</p>
<p>The UN 2030 Agenda is a political commitment, which means that everyone, including libraries and civil society, has a role in making sure all countries meet the goals and leave no one behind. The outcomes are not legally binding. However, the monitoring and follow-up process is taken seriously, and includes both thematic and voluntary national reviews every year as part of the <a href="https://sustainabledevelopment.un.org/hlpf">UN High-Level Political Forum</a>.</p>
<p>There are two things I want you to keep in mind throughout this presentation:</p>
<ul>
<li>Agenda 2030 is more than the SDGs</li>
<li>The purpose of the SDGs or Agenda 2030 is for <em>UN Member States</em> to achieve the Goals</li>
</ul>
</section>
<section id="origins-of-the-sdgs" class="level2">
<h2 class="anchored" data-anchor-id="origins-of-the-sdgs">Origins of the SDGs</h2>
<p>A little bit of the backstory about where the new Agenda came from helps set the context for what the UN is trying to achieve.</p>
<p>Before the SDGs came the <a href="https://www.un.org/millenniumgoals/">Millennium Development Goals</a> that ended in 2015. Success in achieving the MDGs was very mixed. As the UN neared the end of the MDG process, various conferences and documents began to lay the groundwork to negotiate the next process. Starting in 2012, this became known as the <a href="https://sustainabledevelopment.un.org/post2015">“post-2015 process”</a>. From the outset, the post-2015 process was intended to be much more inclusive than the MDGs, which were very top-down.</p>
<p>This meant that civil society would be able to play a much bigger role than before in advocacy and helping to shape the new goals. Over three years, this was an extremely intensive and political process.</p>
<p><a href="https://doi.org/10.1177/0340035216647393">Libraries got involved in the process early on</a>. IFLA took the lead as they have <a href="https://www.un.org/ecosoc/en/ngo/consultative-status">consultative status with ECOSOC</a> which means they are allowed to register to participate in UN meetings as part of the civil society delegation. The work was largely led by a couple of staff at IFLA in close collaboration with the Board, and two US-based Governing Board members who could attend all the UN meetings in New York.</p>
<p>I’m able to share this origin story with you because I was one of those staff members and my job was to respond to consultations, <a href="https://www.ifla.org/news/cape-town-declaration-2015/">prepare briefings for Ministers</a>, <a href="https://web.archive.org/web/20210505163228/https://www.ifla.org/node/8378">build coalitions with other NGOs</a>, train librarians to respond in their own countries, and write <a href="https://repository.ifla.org/handle/123456789/243">advocacy toolkits</a>. And I also got my turn to attend sessions at the UN in New York, and other conferences with stakeholders and Ministers. It was a huge amount of work, but a lot of fun too.</p>
<p>So why did libraries get involved so early? Past experience had shown that the process matters: documents and positions are built up over many iterations and a long period of time. If we wanted our issues to be part of the agenda, libraries needed to have a seat at the table from the start. The other reason is that the 2030 Agenda would have some influence in shaping where governments allocate their attention and resources, and this was a huge opportunity to build the case for investing in access to information and libraries that colleagues could call on in their country.</p>
</section>
<section id="outcomes" class="level2">
<h2 class="anchored" data-anchor-id="outcomes">Outcomes</h2>
<p>At the end of all this advocacy, there are three key achievements in the 2030 Agenda that are relevant to access to information and open access specifically:</p>
<ul>
<li><p>First, universal literacy is recognised in the vision for the UN 2030 Agenda.</p></li>
<li><p>Second, working in collaboration with a huge range of civil society partners and UN Member States, access to information was recognised in the SDGs [Goal 16 Target 16.10]: “Ensure public access to information and protect fundamental freedoms, in accordance with national legislation and international agreements”.</p></li>
<li><p>And, third, with the support of colleagues in a few key countries indicator 16.10.2 “Number of countries that adopt and implement constitutional, statutory and/or policy guarantees for public access to information”. The indicators matter because these help to track progress towards meeting the Goals.</p></li>
</ul>
<p>Apart from access to information, libraries were also strong supporters of targets for access to the internet and ICTs, and culture. Both of which in turn also contribute to open access, because without access to the internet you cannot access information online, and without culture you cannot recognize the value of local and multilingual knowledge.</p>
</section>
<section id="framing-access-to-information" class="level2">
<h2 class="anchored" data-anchor-id="framing-access-to-information">Framing access to information</h2>
<p>The 17 SDGs address issues like land, water, poverty, education, and industry. Some of these are less controversial than others. The Goal on Peace and Justice was more challenging for Member States to agree on, and for a time was at risk of being dropped entirely. The language about public access to information changed multiple times. An additional challenge is that many civil society organisations define access to information only in the context of Right to Information laws or Freedom of Information but it was really critical that it consider access more broadly. The rationale for this was set out in the <a href="https://www.lyondeclaration.org/">Lyon Declaration on Access to Information and Development</a>. It was a great success that it was finally adopted in the form you see today.</p>
<p>You might have noticed some words are not included in any of these statements. Nowhere are there any words about libraries, or open access. That’s because in a process as high-level and consensus-driven as this one, you have to identify language and terms that will get other stakeholders on board. There’s also a lot of advocacy to get other NGOs, media organisations, rights activists on board before you can make the case to government. The key is then to use those coalitions to advocate for what you want to achieve more specifically.</p>
<p>This framing and language about access to information also sits within a larger human rights-based context. Many NGOs and campaigners work on issues like freedom of expression and access to information, and have argued that access to information is necessary to democracy and underpins all other rights. So you can hear echoes of that approach when we talk about things like access to information and open access underpinning the achievement of all the other goals.</p>
</section>
<section id="the-contribution-of-open-access-to-the-sdgs" class="level2">
<h2 class="anchored" data-anchor-id="the-contribution-of-open-access-to-the-sdgs">The contribution of open access to the SDGs</h2>
<p>While the 2030 Agenda does not include the phrase open access, it has an obvious connection to target 16.10. Public access to information through means such as open access helps people to exercise their civil and political rights, be economically active, learn new skills, enrich cultural identity, and take part in decisionmaking (as outlined in the <a href="https://www.lyondeclaration.org/">Lyon Declaration</a>).</p>
<p>However, as I mentioned before, the 2030 Agenda is broader than the SDGs and includes a range of related processes to monitoring, financing, and partnerships to support the Goals. In other words, who is going to pay for it, and how will we know when the Goals have been met.</p>
<p>The UN recognised that research and data were critical to this understanding. In 2015, UN recommended the development of an open access knowledge platform to bring together the scientific knowledge that underpins the SDGs. <a href="https://www.ifla.org/wp-content/uploads/2019/05/assets/hq/topics/libraries-development/documents/ifla_response_to_addis_ababa_agenda_for_action.pdf">IFLA welcomed this recommendation at the time</a>, and it’s very encouraging that the <a href="https://tfm2030connect.un.org/">platform</a> is now under development and harvests research from existing Open Access repositories as well as and UN agencies. This demonstrates high-level, global recognition of the value of open access in meeting global challenges.</p>
<p>This example also highlights that although the SDGs are the part of the 2030 Agenda most people know best, all parts of the Agenda are relevant. Not many people would have expected open access (or in UN language, Technology Facilitation Mechanism) to turn up in the discussions in Addis Ababa about what it would cost to implement the SDGs.</p>
<p>Since the SDGs were adopted, we’ve seen great strides in open access and open science in intergovernmental agencies, many of which have adopted <a href="http://www.unesco.org/new/fileadmin/MULTIMEDIA/HQ/ERI/pdf/oa_policy_rev2.pdf">open access policies</a> and <a href="https://wiki.creativecommons.org/wiki/Intergovernmental_Organizations">licences</a> for their own outputs, the growth of <a href="https://data.un.org/">UN data platforms</a>, and also the <a href="https://en.unesco.org/science-sustainable-future/open-science/recommendation">draft recommendation on open science at UNESCO</a>. This is fantastic progress in a short period of time.</p>
</section>
<section id="how-much-progress-has-been-made-since-2015" class="level2">
<h2 class="anchored" data-anchor-id="how-much-progress-has-been-made-since-2015">How much progress has been made since 2015?</h2>
<p>That’s the background to the process and what was achieved in 2015, and some of the high-level achievements in open access since that time. We’re now five years in with less than a decade until 2030. What else has been achieved since then?</p>
<p>The monitoring process is a critical part of identifying whether UN Member States are on track to meet the goals.</p>
<p>Stepping back a little bit from the SDGs specifically, indicators and data are a critical way that civil society can hold governments to account for progress on any number of issues. In the human rights and access to information space, there are numerous reports that track progress on press freedom, access to the internet, regime types and so on. For example, these include <a href="https://freedomhouse.org/report/freedom-world">Freedom in the World report</a> from Freedom House, <a href="https://rsf.org/en/ranking_table">World Press Freedom Index</a> statistics. There are also data compiled by intergovernmental organisations from official statistics, such as the <a href="https://www.itu.int/en/ITU-D/Statistics/Pages/stat/default.aspx">ITU</a> that tracks mobile phone and internet access, and the <a href="https://info.worldbank.org/governance/wgi/">World Bank Governance Indicators</a>. While it might seem that we are awash in research reports and open data from everywhere, at the start of the SDGs the UN was concerned that many countries still lacked the ability to provide data on basic information such as the number of births in their country. That type of very basic gap means that it is very difficult to identify who needs services.</p>
<p>So this is why monitoring and data has become a very important part of the process. Without data, we cannot really understand whether progress has been made. Different UN agencies are responsible for monitoring the 232 indicators in the SDGs. The main event where these data are reported are the annual <a href="https://sustainabledevelopment.un.org/hlpf">UN High-Level Political Forum</a>.</p>
<p>So knowing that one of the main activities of some civil society organisations is to produce data and reports, that’s exactly what many have done for the SDGs to help complement official statistics and reports. For example, I helped to establish the <a href="https://da2i.ifla.org/">Development and Access to Information report, DA2I</a>. But many groups also contribute directly to the official monitoring processes which are called <a href="https://sustainabledevelopment.un.org/vnrs/">Voluntary National Reviews</a> and thematic reviews.</p>
<p>Everyone can make a direct contribution to sharing their story about how open access makes a difference as part of this process. For example, <a href="https://read.alia.org.au/sustainable-development-goals">ALIA</a>, <a href="https://www.librariesaotearoa.org.nz/korero-blog/libraries-and-sdgs-an-update">LIANZA</a>, and <a href="https://www.caul.edu.au/news/australian-university-libraries-and-united-nations-sustainable-development-goals-sdgs">CAUL</a> have been active in this topic on behalf of libraries in Australia and Aotearoa New Zealand. This advocacy has paid off with libraries recognised in <a href="https://sdgs.org.au/wp-content/uploads/2018/06/AustralianVNR_2018_final.pdf">Australia’s first National Voluntary Review</a>, libraries were also a part of the people’s review in New Zealand.</p>
<p>Groups of countries are asked to report back on their progress each year. IFLA recently analysed the <a href="https://www.ifla.org/publications/libraries-engaged-in-voluntary-national-reviews-vnrs/">Voluntary National Reviews submitted to date</a>, and found that nearly half of the reports presented in 2021 mentioned libraries or information. This shows just how possible it is to make impact at a high level, and how useful and important it is that if you are working on the SDGs to ensure you share your stories.</p>
<p>A <a href="https://librarymap.ifla.org/storytelling-manual">story</a> could be very big, or it could be much more local. It’s the impact that matters. For example, stories about open access might look at how access to research helped identify existing treatments that would be useful for COVID19, or make more visible neglected tropical diseases that lack sufficient funding and attention. It could look at how open access supports student start ups, or how a researcher at your institution was able to reach more readers in different countries than before. Your story could be about the difference your whole library makes, or the impact of one researcher, or one discovery, on the lives of others.</p>
<p>Back in 2015, I would sometimes hear that access to information had been solved and was not a problem anymore. This was because everyone has a phone, and lots of countries have Right to Information legislation. But as librarians, we know that the digital divide and gaps in literacy skills exist in every library, community, and country. Since 2015, we have seen the rise of misinformation and disinformation as key concerns in elections, and around the COVID-19 pandemic. Open access and rapid access to scientific outputs from preprints through to published articles and data have become a key means of ensuring people have access to facts and quality information.</p>
<p>At the local level, using stories, libraries globally have embraced the Goals and incorporated them into their work. It’s so rewarding that to see that so many libraries now are engaged with the SDGs when all this started with a very small group of people to build the case for libraries, grow advocates, and provide reusable resources for others to adapt and build on.</p>
<p>Much of this work has involved mapping stories to the goals and showing how open access and access to information underpin each of the Goals in different communities. This is a fantastic way to raise awareness about the role of the Goals in each of our daily lives. Many universities have adopted strategies around Grand Challenges or <a href="https://ap-unsdsn.org/regional-initiatives/universities-sdgs/university-sdg-guide/">undertaking research that supports the SDGs</a>, and this is also a great way to show the impact of research and open access.</p>
<p>But I would also encourage you to go further to share these stories with CAUL, CONZUL, ALIA, LIANZA, and IFLA. It’s so critical to have more of these stories and data at the national level as the official monitoring process continues.</p>
</section>
<section id="what-are-the-sdgs-for" class="level2">
<h2 class="anchored" data-anchor-id="what-are-the-sdgs-for">What are the SDGs for?</h2>
<p>This is important to remember too because it’s important to keep a focus on what the SDGs are for.</p>
<p>In addition to all the official data and reports, and contributions from civil society, in relation to open access and research we’ve seen that all of the main bibliographic tools have mapped research in their indexes to the Goals. Combined with open access filters, this may be useful in getting a sense of how a country or institution’s output maps to the goals, and is similar to an overview of research by FoR code. It can also be useful to identify resources to learn more about different aspects of the SDGs.</p>
<p>But, just my opinion, there are also some activities that perhaps aren’t quite as useful, and that can divert energy away from more meaningful activities. For example, Times Higher Education’s Impact Ranking “are the only global performance tables that assess universities against the United Nations’ Sustainable Development Goals (SDGs).” But unfortunately, the ranking is a missed opportunity to show just how much universities contribute to open access, research, and their communities as it has no meaningful alignment with any official monitoring or review process.</p>
<p>In addition to that major gap, there are some other omissions. Goal 16 would have been a perfect opportunity to include open access, <a href="https://www.timeshighereducation.com/sites/default/files/breaking_news_files/the_impactrankings_methodology_2021_v1.3_final.pdf">but it is missing</a>. But other areas could have referenced open access too - one of the key measures under innovation is Patents citing university research (15.4%). No Poverty has a measure looking at co-authorship with researchers in low income countries. but again, nothing about open access to research. Sustainable Cities and Communities has the closest indicator that could possibly include open access, concerning public access to libraries: “Provide public access to libraries including books and publications”.</p>
<p>So once again, what are the SDGs for? They are not an exercise to rank institutions on how we are doing, but a means of ensuring no one is left behind, and that governments meet their commitments by investing in policies and services that make a difference in people’s lives and that support the planet.</p>
</section>
<section id="monitoring-progress-of-open-access---official-and-unofficial-sources" class="level2">
<h2 class="anchored" data-anchor-id="monitoring-progress-of-open-access---official-and-unofficial-sources">Monitoring progress of open access - official and unofficial sources</h2>
<p>Let’s turn to how open access is tracking in the 2030 Agenda.</p>
<p>We can look at data collected by UNESCO, <a href="https://en.unesco.org/news/access-information-gets-upgrade-sdg-indicators-framework">which is the custodian for 16.10</a> (access to information) data.</p>
<p>We can look at third-party data such as open access rates by country over the last few years. The <a href="https://www.leidenranking.com">Leiden Ranking</a> and various bibliographic databases can source this data.</p>
<p>Plus the subsequent development of instruments like the UNESCO Recommendation on Open Science show just how much commitment and interest there is in open access among governments and UNESCO.</p>
<p>A few other sources of official and unofficial SDG and open access data for national level reporting could include:</p>
<ul>
<li><a href="https://sdg-tracker.org/peace-justice#16.10.2">Our World in Data SDG Tracker</a></li>
<li><a href="https://da2i.ifla.org/da2i-dashboards/">IFLA and TASCHA DA2I dashboards</a></li>
<li><a href="https://www.sdgaccountability.org/report/">TAP Network SDG Accountability Report</a></li>
</ul>
<p>Each year the UN prepares a narrative report on progress towards the Goals. There has been <a href="https://unstats.un.org/sdgs/report/2020/goal-16/">some progress</a> on areas related to open access, access to information and data, but also remaining challenges. The <a href="https://unstats.un.org/sdgs/report/2021/note-to-reader/">most recent report on data</a> under Goal 17 notes the contributions of civil society, the private sector and academia, but that there are many gaps still.</p>
<p>You can also <a href="https://unstats.un.org/sdgs/indicators/database/">dig into the datasets</a> to get a sense of progress. If you want a readymade sense of progress, there’s the <a href="https://unstats.un.org/sdgs/report/2021/progress-chart-2021.pdf">UN stats progress charts</a> which has a series of dials to track how things are going. Unfortunately not all indicators are reported on every year, but I think you will get a sense of the challenges from this year’s report on Goal 16 to see that overall the picture is very mixed. Sadly many indicators are going backwards. Press freedom, internet access are challenges in many countries. Over half the world is still offline.</p>
</section>
<section id="sharing-your-stories" class="level2">
<h2 class="anchored" data-anchor-id="sharing-your-stories">Sharing your stories</h2>
<p>If you are interested to get more involved in looking at how to measure your own contribution to the SDGs, where should you start? It’s really important to remember that the relative importance of different goals varies widely between countries, and even institutions. I often describe the SDGs as just one tool in your toolkit that is more relevant for some advocacy purposes than others. You must consider who you are speaking to and consider whether or not the SDGs are relevant in your context.</p>
<p>So with that in mind, I spoke earlier about the stories you can tell and the data available from different sources to make the case. There is a huge amount of resources and documentation now, I would suggest starting by looking at:</p>
<ul>
<li>Your own library’s plans and objectives - there are a couple of libraries that have already mapped their work in this way</li>
<li>Your institution’s strategic plan, particularly if they mention global challenges or the SDGs</li>
<li>ALIA, LIANZA, CAUL, CONZUL resources</li>
<li><a href="https://ap-unsdsn.org/regional-initiatives/universities-sdgs/university-sdg-guide/">SDSN Universities guide</a></li>
<li><a href="https://www.ifla.org/take-action/">IFLA for the global perspective</a></li>
</ul>
<p>Work with other people to gather stories, data, and think about how you will communicate it. I encourage you to get engaged with the SDGs and other activities like the UNESCO draft recommendation on Open Science to keep the pressure on to make the most of opportunities at every level locally and nationally to show the difference libraries, information, and open access makes. As librarians we live the SDGs every day: we innovate, we educate, we support sustainability, and equity – the SDGs are not just a report card, they are at the heart of what we do. It’s up to us now to tell others about it.</p>


</section>

 ]]></description>
  <category>talks</category>
  <guid>https://www.fionabradley.com/posts/contribution-of-open-access-to-the-un-sustainable-development-goals/index.html</guid>
  <pubDate>Mon, 30 Aug 2021 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Libraries, AI, and the policy and regulatory environment</title>
  <dc:creator>Fiona Bradley</dc:creator>
  <link>https://www.fionabradley.com/posts/libraries-ai-and-the-policy-and-regulatory-environment/index.html</link>
  <description><![CDATA[ 




<section id="libraries-ai-and-the-policy-and-regulatory-environment" class="level2">
<h2 class="anchored" data-anchor-id="libraries-ai-and-the-policy-and-regulatory-environment">Libraries, AI, and the policy and regulatory environment</h2>
<p><em>Talk presented at <a href="https://www.ifla-wlic2021.com/">WLIC2021</a> 18 August 2021</em></p>
<blockquote class="blockquote">
<p>“…but A.I<br>
can solve all our problems<br>
but where do ethics go”<br>
Wang Leehom《A.I. 愛》</p>
</blockquote>
<p>We are already living in an algorithmic society. But, while we now understand more about bias in machine learning sets, the surveillance risks of smart cities and facial recognition (<a href="http://doi.org/10.1038/d41586-020-03187-3">Van Noorden, 2020</a>), upload filtering and content moderation on social networks (<a href="http://doi.org/10.1177/2053951720943234">Gillespie, 2020</a>; <a href="http://doi.org/10.1177/2053951720920686">Llansó, 2020</a>), policies and regulation are still emerging.</p>
<p>Some of the key challenges in AI regulation are that:</p>
<ul>
<li>The intersection between AI, privacy, and human rights is still emerging. It is necessary to make the case for how AI impacts human rights as well as to address equity, ethics, and fairness.</li>
<li>Laws are important, but other forms of regulation or influence apart from top-down rules are also key (<a href="http://doi.org/10.5235/17579961.5.1.1">Moses, 2015</a>). These can include setting research agendas, funding, engagement in standards development, and geopolitical positioning (<a href="http://doi.org/10.1080/17579961.2021.1898300">Smuha, 2021</a>).</li>
<li>“AI” as a concept is as broad and difficult to define as “data”, or “information”, and thus cannot be readily confined to specific applications. This is evident in different approaches to regulation to date, whether risk-based, or sector-based.</li>
</ul>
<p>The current policy and regulatory landscape is evolving in multiple ways:</p>
<ul>
<li>Along broad values and rights-based lines: trustworthy, responsible, ethical AI are all terms describing a similar approach (Smuha, 2021) that in some cases are also evolving into more binding regulation. In Europe, for example, guidelines for trustworthy AI were adopted in 2019 and proposed harmonised rules for risk-based regulation in 2021 (<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206">European Parliament, 2021</a>). Australia has developed a rights-based approach and adopted a voluntary AI and ethics framework (<a href="https://tech.humanrights.gov.au/sites/default/files/2021-05/AHRC_RightsTech_2021_Final_Report.pdf">Australian Human Rights Commission, 2021</a>; <a href="https://www.industry.gov.au/data-and-publications/">Department of Industry, Science, Energy, and Resources, 2021</a>)</li>
<li>At the same time, AI has become a point of geopolitical positioning and competition between Europe, the US, and China and other countries that all seek to lead in AI innovation, and regulation (Smuha, 2021). This results in policies and plans that incorporate issues such as research investment, intellectual property, export controls, but that also seek to influence. China’s New Generation Artificial Intelligence Development Plan incorporates ethical considerations, but also research funding and education (<a href="http://doi.org/10.1007/s00146-020-00992-2">Roberts et al., 2020</a>; <a href="http://doi.org/10.1038/s42256-020-0183-4">Wu et al., 2020</a>). In Europe, the proposed regulation seeks to influence state and corporate behaviour (<a href="https://www.scl.org/articles/12278-the-eu-draft-ai-regulation-what-you-need-to-know-now">Tanna, 2021</a>).<br>
</li>
<li>Other countries such as the US are pursuing a sectoral approach (<a href="https://www.brookings.edu/research/ai-needs-more-regulation-not-less/">MacCarthy, 2020</a>). This reflects the vast difference between AI powered search engines (less risk), algorithmic decision-making by government (<a href="http://www.theguardian.com/australia-news/2021/jun/11/robodebt-court-approves-18bn-settlement-for-victims-of-governments-shameful-failure">Henriques-Gomes, 2021</a>), and military applications (high risk).</li>
<li>Bodies like the OECD and UNESCO are also looking at regulation across multiple countries (Smuha, 2021)</li>
<li>And, there is renewed attention to other forms of regulation including technical and professional standards (<a href="http://doi.org/10.1093/ijlit/eaw012">Rachovitsa, 2016</a>; <a href="http://doi.org/10.1017/S1752971915000081">Raymond &amp; DeNardis, 2015</a>).</li>
</ul>
<p>So, what does all this mean for libraries? Regulation will shape the types of technologies available in different countries and industries, the ways that people can seek remedies if their rights are violated, and the societies that we will live in. In this context, it is not surprising that librarians have a range of views about AI, from embracing its potential to deep scepticism and alarm. Meanwhile, many vendors and startups based on library metadata are already promoting AI as part of their products. But there is not yet enough disclosure or transparency about how these features really work, nor what ethical standards vendors are holding themselves to. More debate is needed to build consensus about where the profession stands in relation to these issues, and it is important to incorporate existing national ethical AI frameworks into procurement decisions, and guidance about these issues to library users.</p>
<p>In Australia, The Australian Library and Information Association (ALIA) responded to the consultation on Australia’s voluntary AI framework observing that AI provides opportunities for service improvement as well as potential threats. They noted that, “Library and information professionals will need training and ongoing learning to enable us to understand and apply principles for ethical AI in our business practices.”(<a href="https://consult.industry.gov.au/strategic-policy/artificial-intelligence-ethics-framework/consultation/view_respondent?show_all_questions=0&amp;sort=submitted&amp;order=ascending&amp;_q__text=library&amp;uuId=616546801">Australian Library and Information Association, 2019</a>). According to current ALIA guidelines, entry-level librarians in Australia are expected to know about AI and ML.</p>
<p>High profile failures in algorithmic decision-making by government means that some of the potential risks associated with these technologies are already widely known in Australia. A massive failure that became known as “Robodebt” used algorithmic decision-making to identify people who received money from Centrelink, a government public benefit program, and incorrectly matched their information with other datasets in the Australian Taxation Office. As a result, hundreds of thousands of people received letters falsely stating that they owed huge debts to the government. In June 2021, a $1.8 billion settlement was reached to resolve the matter (Henriques-Gomes, 2021).</p>
<p>There is also a very large program of research work on algorithmic fairness in Australia from a wide range of perspectives. These include the ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S), and a significant report on Human Rights and Technology released in 2021 which recommends accountability mechanisms for government and corporations, and compliance with anti-discrimination laws to promote algorithmic fairness (Australian Human Rights Commission, 2021).</p>
<p>Incorporating these national level recommendations provides a robust foundation for the ways in which libraries can contribute to a more fair society and work with vendors to protect user privacy.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Australian Human Rights Commission. (2021). Human rights and technology. Retrieved from <a href="https://tech.humanrights.gov.au/sites/default/files/2021-05/AHRC_RightsTech_2021_Final_Report.pdf" class="uri">https://tech.humanrights.gov.au/sites/default/files/2021-05/AHRC_RightsTech_2021_Final_Report.pdf</a></p>
<p>Australian Library and Information Association. (2019). Australia’s AI Ethics Framework Response 616546801. Retrieved from <a href="https://consult.industry.gov.au/strategic-policy/artificial-intelligence-ethics-framework/consultation/view_respondent?show_all_questions=0&amp;sort=submitted&amp;order=ascending&amp;_q__text=library&amp;uuId=616546801" class="uri">https://consult.industry.gov.au/strategic-policy/artificial-intelligence-ethics-framework/consultation/view_respondent?show_all_questions=0&amp;sort=submitted&amp;order=ascending&amp;_q__text=library&amp;uuId=616546801</a></p>
<p>Department of Industry, Science, Energy and Resources. (2021, 17 June 2021). Australia’s AI Ethics Principles. Department of Industry, Science, Energy and Resources. Retrieved from <a href="https://www.industry.gov.au/data-and-publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles" class="uri">https://www.industry.gov.au/data-and-publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles</a></p>
<p>European Parliament. (2021, 21 April 2021). Proposal for a Regulation of The European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts. EUR-Lex. Retrieved from <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206" class="uri">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206</a></p>
<p>Gillespie, T. (2020). Content moderation, AI, and the question of scale. Big Data &amp; Society, 7(2), 205395172094323. <a href="doi:10.1177/2053951720943234" class="uri">doi:10.1177/2053951720943234</a></p>
<p>Henriques-Gomes, L. (2021, 11 June 2021). Robodebt: court approves $1.8bn settlement for victims of government’s ‘shameful’ failure. The Guardian. <a href="http://www.theguardian.com/australia-news/2021/jun/11/robodebt-court-approves-18bn-settlement-for-victims-of-governments-shameful-failure" class="uri">http://www.theguardian.com/australia-news/2021/jun/11/robodebt-court-approves-18bn-settlement-for-victims-of-governments-shameful-failure</a></p>
<p>Llansó, E. J. (2020). No amount of “AI” in content moderation will solve filtering’s prior-restraint problem. Big Data &amp; Society, 7(1), 2053951720920686. <a href="doi:10.1177/2053951720920686" class="uri">doi:10.1177/2053951720920686</a></p>
<p>MacCarthy, M. (2020, 2020/03/09/). AI needs more regulation, not less. Retrieved from <a href="https://www.brookings.edu/research/ai-needs-more-regulation-not-less/" class="uri">https://www.brookings.edu/research/ai-needs-more-regulation-not-less/</a></p>
<p>Moses, L. B. (2015). How to Think about Law, Regulation and Technology: Problems with ‘Technology’ as a Regulatory Target. Law, Innovation and Technology, 5(1), 1-20. <a href="doi:10.5235/17579961.5.1.1" class="uri">doi:10.5235/17579961.5.1.1</a></p>
<p>Rachovitsa, A. (2016). Engineering and lawyering privacy by design: understanding online privacy both as a technical and an international human rights issue. International Journal of Law and Information Technology, 24(4), 374-399. <a href="doi:10.1093/ijlit/eaw012" class="uri">doi:10.1093/ijlit/eaw012</a></p>
<p>Raymond, M., &amp; DeNardis, L. (2015). Multistakeholderism: anatomy of an inchoate global institution. International Theory, 7(3), 572-616. <a href="doi:10.1017/S1752971915000081" class="uri">doi:10.1017/S1752971915000081</a></p>
<p>Roberts, H., Cowls, J., Morley, J., Taddeo, M., Wang, V., &amp; Floridi, L. (2020). The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation. Ai &amp; Society, 36(1), 59-77. <a href="doi:10.1007/s00146-020-00992-2" class="uri">doi:10.1007/s00146-020-00992-2</a></p>
<p>Smuha, N. A. (2021). From a ‘race to AI’ to a ‘race to AI regulation’: regulatory competition for artificial intelligence. Law, Innovation and Technology, 13(1), 57-84. <a href="doi:10.1080/17579961.2021.1898300" class="uri">doi:10.1080/17579961.2021.1898300</a></p>
<p>Tanna, M. (2021, 2021/06/04/). The EU Draft AI Regulation: what you need to know now. Retrieved from <a href="https://www.scl.org/articles/12278-the-eu-draft-ai-regulation-what-you-need-to-know-now" class="uri">https://www.scl.org/articles/12278-the-eu-draft-ai-regulation-what-you-need-to-know-now</a></p>
<p>Van Noorden, R. (2020). The ethical questions that haunt facial-recognition research. Nature, 587(7834), 354-358. <a href="doi:10.1038/d41586-020-03187-3" class="uri">doi:10.1038/d41586-020-03187-3</a></p>
<p>Wu, F., Lu, C., Zhu, M., Chen, H., Zhu, J., Yu, K., . . . Pan, Y. (2020). Towards a new generation of artificial intelligence in China. Nature Machine Intelligence, 2(6), 312-316. <a href="doi:10.1038/s42256-020-0183-4" class="uri">doi:10.1038/s42256-020-0183-4</a></p>


</section>

 ]]></description>
  <category>talks</category>
  <guid>https://www.fionabradley.com/posts/libraries-ai-and-the-policy-and-regulatory-environment/index.html</guid>
  <pubDate>Tue, 17 Aug 2021 14:00:00 GMT</pubDate>
</item>
</channel>
</rss>
